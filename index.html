
<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Zhe Wang</title>
  <link rel="shortcut icon" href="logo.jpg">
	<meta content="Yu (Ryan) Gong, yugong95.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'wangzheallen.github.io');
  ga('send', 'pageview');

</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Zhewang" style="float: left; padding-left: .01em; height: 140px;" src="wz2.jpg" />
<div style="padding-left: 10em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Zhe Wang (Allen)</span><br />
<span><a href='http://vision.ics.uci.edu/'>Computational Vision Group</a></span> <br />
<span>UCI, Irvine</span><br />
<span><strong>Office</strong>: 4209 Bren Hall, Irvine, CA</span><br />
<span><strong>Email  </strong>: buptwangzhe2012 [at] gmail [dot] com</span> <br />
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<h2>About Me (<a href='WangZheCV_20160913.pdf'>CV</a>) (<a href='https://scholar.google.com.hk/citations?user=FL-t3JEAAAAJ&hl=en'>Google Scholar</a>)</h2>
<div class="paper">
I am a computer science PhD student at <a href = 'http://www.ics.uci.edu/'>Donald Bren School of Information and Computer Sciences, UC Irvine</a>. I work at the <a href='http://vision.ics.uci.edu/'>Computational Vision Group</a> where I am advised by <a href='http://www.ics.uci.edu/~fowlkes/'>Prof. Charless Fowlkes</a>.
<br> <br>


Before coming to UCI, I was a joint research assistant of <a href = 'http://www.ie.cuhk.edu.hk/main/index.shtml'>Department of Information Engineering</a> at <a href='http://www.cuhk.edu.hk/chinese/index.html'>The Chinese University of Hong Kong</a> and <a href='http://www.siat.cas.cn/jgsz/kyxt/jcs/yjdy/dmtjc/'>Multimedia Research Center</a> at <a href='http://english.siat.cas.cn/'>Shenzhen Institute of Advanced Technology</a> under the supervision of Prof. <a href='http://mmlab.siat.ac.cn/yuqiao/'>Yu Qiao</a> and supported by Prof. <a href='http://www.ie.cuhk.edu.hk/people/xotang.shtml'>Xiaoou Tang</a> and Prof. <a href='http://personal.ie.cuhk.edu.hk/~ccloy/'>Change LOY Chen</a>.
I got my B.S. degree from <a href='http://english.bupt.edu.cn/'>Beijing University of Posts and Telecommunications (BUPT)</a>. I finished my B.S. graduation thesis in EE department in Tsinghua university under the supervision of <a href='http://www.ee.tsinghua.edu.cn/publish/ee/4157/2010/20101217175105739407380/20101217175105739407380_.html'>Prof. Guangda Su</a>.
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
    <li> February 4, 2017: Our VSAD paper is accepted by TIP and feature for MIT indoor (acc 86.1) is released. Project Page is also constructed</li>
    <li> July 15, 2016: One paper is accepted by ECCV 2016.</li>
    <li> July 9, 2016: Project website for CVPR'16 paper is online.</li>
    <li> June 16, 2016: Our team <I>CUHK & ETHZ & SIAT</I> won the first place in <a href='http://activity-net.org/challenges/2016/index.html'>ActivityNet Large Scale Activity Recognition Challenge 2016</a>! This year the challenge is hosted together with CVPR'16.</li>
    <li> March 24, 2016: I gave an oral presentation on <a href='http://www.icassp2016.org/'>ICASSP 2016</a> in Shanghai. here is the <a href='papers/ICASSP_slides.pdf'>slides</a>.</li>
    <li> March 1, 2016: One paper is accepted by CVPR 2016.</li>
    <li> January 27, 2016: Awarded <a href='http://baike.baidu.com/link?url=462kNWmmwbBuDqsdzj2ayy8QqFPes27F0JErLEKUvVitZxfPBOF-dXGuNGtg1rFQBEgq5UcSeDZagxaWFFvNpa'>CAS Dean's Outstanding Scholarship</a></li>
    <li> December 12, 2015: I gave an oral presentation on ICCV <a href='http://gesture.chalearn.org/mmworkshop'>ChaLearn</a> workshop, here is the <a href='papers/WangWGQ_ChaLearnLAP15_slide.pdf'>slides</a>. </li>
	  <li> March 29, 2015: We are the 1st winner of both tracks for action recognition and cultural event recognition, on ChaLearn <a href='http://gesture.chalearn.org/'>Looking at People Challenge</a> at CVPR 2015. </li>
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Publications</h2>


<div class="paper" id="tip_scene"><img class="paper" src="papers/arxiv_tip.png" title="Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition" />
<div> <strong>Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition</strong><br />
<strong>Z. Wang</strong>, L. Wang, Y. Wang, B. Zhang, and Y. Qiao <br />
IEEE Transactions on Image Processing (<strong>TIP</strong>), 2017, in press.<br />
[ <a href='http://arxiv.org/abs/1609.00153'>Paper</a>  ]  [ <a href='https://github.com/wangzheallen/vsad'>Code& Model</a>  ] [ <a href='http://mmlab.siat.ac.cn/mit_hybrid_vsad.mat'>Feature</a>  ] [ <a href='http://wangzheallen.github.io/vsad'>Project Page</a>  ]<br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ijcv_event"><img class="paper" src="papers/arxiv_ijcv2.png" title="Transferring Object-Scene Convolutional Neural Networks for Event Recognition in Still Images" />
<div> <strong>Transferring Object-Scene Convolutional Neural Networks for Event Recognition in Still Images</strong><br />
L. Wang, <strong>Z. Wang</strong>, Y. Qiao, and L. Van Gool <br />
submitted to (<strong>IJCV</strong>).<br />
[ <a href='http://arxiv.org/abs/1609.00162'>Paper</a>  ]  [ <a>Code</a>  ]<br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ECCV2016"><img class="paper" src="papers/WangXWQ_ECCV16.png" title="Temporal Segment Networks: Towards Good Practices for Deep Action Recognition" />
<div> <strong>Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</strong><br />
L. Wang, Y. Xiong, <strong>Z. Wang</strong>, Y. Qiao, D. Lin, X. Tang, and L. Van Gool <br />
in European Conference on Computer Vision (<strong>ECCV</strong>), 2016.<br />
[ <a href='http://arxiv.org/abs/1608.00859'>Paper</a>  ]  [ <a href='http://wanglimin.github.io/papers/WangXWQLTV_ECCV16.bib'>BibTex</a> ] [ <a>Poster</a> ] [ <a href='https://github.com/yjxiong/temporal-segment-networks'>Code</a>  ]<br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="XiongW_Anet16"><img class="paper" src="papers/XiongW_Anet16_r.png" title="CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016" />
<div> <strong>CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016</strong><br />
Y. Xiong, L. Wang, <strong>Z. Wang</strong>, B. Zhang, H. Song, W. Li, D. Lin, Y. Qiao, L. Van Gool, and X. Tang  <br />
ActivityNet Large Scale Activity Recognition Challenge, in conjuction with <strong>CVPR</strong>, 2016. <br />
[ <a href='papers/XiongW_Anet16.pdf'>Paper</a>  ]  [ <a>BibTex</a> ] [ <a>Presentation</a> ] [ <a href='https://github.com/yjxiong/anet2016-cuhk'>Code</a>  ] <br />
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="CVPR2016"><img class="paper" src="papers/ZhangWWQW_CVPR16.jpg" title="Real-time Action Recognition with Enhanced Motion Vector CNNs" />
<div> <strong>Real-time Action Recognition with Enhanced Motion Vector CNNs</strong><br />
B. Zhang, L. Wang, <strong>Z. Wang</strong>, Y. Qiao, and H. Wang <br />
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2016.<br />
[ <a href='papers/ZhangWWQW_CVPR16.pdf'>Paper</a>  ]  [ <a href='papers/ZhangWWQW_CVPR16.bib'>BibTex</a> ] [ <a href='http://zbwglory.github.io/MV-CNN/index.html'>Project Page</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICASSP2016"><img class="paper" src="papers/ICASSP_750300.png" title="Codebook Enhancement of VLAD Representation for Visual Recognition" />
<div> <strong>Codebook Enhancement of VLAD Representation for Visual Recognition </strong> ( <alert>Oral</alert> )<br />
<strong>Z. Wang</strong>, Y. Wang, L. Wang, and Y. Qiao <br />
IEEE International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2016. <br />
[ <a href='papers/ICASSP.pdf'>Paper</a>  ]  [ <a href='papers/WangWWQ_ICASSP16.bib'>BibTex</a> ] [ <a href='papers/ICASSP_slides.pdf'>Presentation</a> ] [ <a href='papers/ICASSP16_poster.pdf'>Poster</a> ] <br />
<!--<alert>Improve codebook generation quality and get better result on PASCAL VOC07 and HMDB51 than baseline.</alert>-->
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="ICCVW15"><img class="paper" src="papers/LAP.jpg" title="Better Exploiting OS-CNNs for Better Event Recognition in Images" />
<div> <strong>Better Exploiting OS-CNNs for Better Event Recognition in Images</strong><br />
L. Wang, <strong>Z. Wang</strong>, S. Guo, and Y. Qiao <br />
ChaLearn Looking at People (<strong>LAP</strong>) workshop, <strong>ICCV</strong>, 2015. <br />
[ <a href='http://arxiv.org/abs/1510.03979'>Paper</a>  ]  [ <a href='contests/WangWGQ_ChaLearnLAP15.bib'>BibTex</a> ] [ <a href='papers/WangWGQ_ChaLearnLAP15_slide.pdf'>Presentation</a> ] <br />
<!--<alert>Obtain 84.7% mAP and rank 3rd on the track of cultural event recognition.</alert>-->
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangXWQ15"><img class="paper" src="papers/WangXWQ15.jpg" title="Towards Good Practices for Very Deep Two-Stream ConvNets" />
<div> <strong>Towards Good Practices for Very Deep Two-Stream ConvNets</strong><br />
L. Wang, Y. Xiong, <strong>Z. Wang</strong>, and Y. Qiao <br />
ArXiv 1507.02159, 2015. <br />
[ <a href='papers/WangXWQ15.pdf'>Paper</a>  ]  [ <a href='papers/WangXWQ15.bib'>BibTex</a> ] [ <a href='https://github.com/yjxiong/caffe/tree/action_recog'>Code</a> ] <br />
<!--<alert>Obtain 91.4% accuracy on the UCF101 dataset. Caffe extension for Multi-GPU training released.</alert>-->
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangWDQ_LAP15"><img class="paper" src="papers/WangWDQ_ChaLearnLAP15.jpg" title="Event Recognition Using Object-Scene Convolutional Neural Networks" />
<div> <strong>Object-Scene Convolutional Neural Networks for Event Recognition in Images</strong><br />
L. Wang, <strong>Z. Wang</strong>, W. Du, and Y. Qiao <br />
ChaLearn Looking at People (<strong>LAP</strong>) workshop, <strong>CVPR</strong>, 2015. <br />
[ <a href='papers/WangWDQ_ChaLearnLAP15.pdf'>Paper</a>  ]  [ <a href='papers/WangWDQ_ChaLearnLAP15.bib'>BibTex</a> ] [ <a href='papers/WangWDQ_ChaLearnLAP15_slide.pdf'>Presentation</a> ] [ <a href = 'cultural_event/index.html'>Project Page</a> ] <br />
<!--<alert>Obtain 85.5% mAP and rank 1st on the track of cultural event recognition.</alert>-->
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangQT_CVPR15"><img class="paper" src="papers/AR_Zhe2.png" title="Exploring Fisher Vector and Deep Networks for Action Spotting" />
<div> <strong>Exploring Fisher Vector and Deep Networks for Action Spotting</strong><br />
<strong>Z. Wang</strong>, L. Wang, W. Du, and Y. Qiao <br />
ChaLearn Looking at People (<strong>LAP</strong>) workshop, <strong>CVPR</strong>, 2015. <br />
[ <a href='papers/05.pdf'>Paper</a>  ]  [ <a href='papers/05.bib'>BibTex</a> ] [ <a href='papers/05_slides.pdf'>Presentation</a> ]  <br />
<!--<alert>Obtain Jaccard Index of 0.5385 and rank 1st on the track of action/interaction recognition.</alert>-->
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangQT_CVPR15"><img class="paper" src="papers/thumos.gif" title="CUHK&SIAT Submission for THUMOS15 Action Recognition Challenge" />
<div> <strong>CUHK&SIAT Submission for THUMOS15 Action Recognition Challenge</strong><br />
L. Wang, <strong>Z. Wang</strong>, Y. Xiong, and Y. Qiao <br />
THUMOS Challenge 2015, <strong>CVPR</strong> 2015. <br />
[ <a href='papers/THUMOS.pdf'>Paper</a>  ]  [ <a href='papers/THUMOS.bib'>BibTex</a> ]  <br />
<!--<alert>One of the top performers in Action Classification Task.</alert>-->
</div>
<div class="spanner"></div>
</div>

</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professional activities</h2>
<div class="paper">
<ul>
<p><font size="5">Reviewer for <a href='http://pamitc.org/iccv15/'>ICCV 2015 </a>workshop</font></p>
</ul>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Contests</h2>
<div class="paper">
<ul>
<li><strong>ActivityNet Large Scale Activity Recognition Challenge 2016: <a href='http://activity-net.org/challenges/2016/index.html'>Untrimmed Video Classification</a></strong> (<small>in conjunction with CVPR 2016</small>),  <strong>Rank</strong>: 1st place</li>
<li><strong>ChaLearn Looking at People Challenge: <a href='http://gesture.chalearn.org/'>Cultural Event Classification</a></strong> (<small>in conjunction with ICCV 2015</small>),  <strong>Rank</strong>: 3rd place</li>
<li><strong>THUMOS'15 Challenge: <a href='http://www.thumos.info/results.html'>Action Classification</a></strong> (<small>in conjunction with CVPR 2015</small>),  <strong>Rank</strong>: 5th place</li>
<li><strong>ChaLearn Looking at People Challenge: <a href='http://gesture.chalearn.org/2015-looking-at-people-cvpr-challenge'>Cultural Event Classification</a></strong> (<small>in conjunction with CVPR 2015</small>),  <strong>Rank</strong>: 1st place</li>
<li><strong>ChaLearn Looking at People Challenge: <a href='http://gesture.chalearn.org/2015-looking-at-people-cvpr-challenge'>Action/Interaction Classification</a></strong> (<small>in conjunction with CVPR 2015</small>), <strong>Rank</strong>: 1st place</li>
<li><strong>First Audio and Video Competition: <a href='http://avid.erangelab.com/'>Video Classification</a></strong>,  <strong>Rank</strong>: 5th place</li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>
<!--
<div style="clear: both;">
<div class="section"><h2>Awards</h2>
<div class="paper">
<li><strong>“CAS Dean's Outstanding Scholarship”, 2015/2016</a></li>
<li><strong>“Research Assistantship from CUHK-MMLAB”, 2015/2016</a></li>
<li><strong>“Outstanding Board Manger for bbs.byr.cn”, <a href='http://bbs.byr.cn/#!bmvote/view/1'>2014/2015（3/31）</a></li>
<li><strong>“Second-class JDSU Scholarship”, 2013/2014（10/231）</li>
<li><strong>“Second-class Scholarship”, 2013/2014</li>
<li><strong>“Merit Student”, 2013/2014</li>
<li><strong>“Third-class Scholarship”, 2012/2013</li>
<li><strong>“Excellent Student Cadres”, 2012/2013</li>
</div>
</div>
</div>
-->

<div style="clear: both;">
<div class="section"><h2>Friends && Collaborators</h2>
<div class="paper">
<a href='http://bestsonny.github.io/'>Pan He</a> (UFL),
<a href='http://www.wlhuang.org/'>Weilin Huang</a> (Oxford),
<a href='http://simonchanper.github.io/'>Xiang Chen</a> (Darmstadt),
<a href='http://kpzhang93.github.io/'>Kaipeng Zhang</a> (NTU Taiwan),
<a href='http://tonghe90.github.io/'>Tong He</a> (WHU),
<a href='http://ydwen.github.io'>Yandong Wen</a> (CMU&SYSU),
<a href='http://guoshengcv.github.io/'>Sheng Guo</a> (SIAT), <a href='http://zbwglory.github.io/'>Bowen Zhang</a> (Tongji University), <a href='http://wanglimin.github.io/'>Limin Wang</a> (ETHz), <a href='http://personal.ie.cuhk.edu.hk/~xy012/'>Yuanjun Xiong</a> (CUHK), Yali Wang</a> (SIAT), <a href='http://mmlab.siat.ac.cn/yuqiao/index.html'>Yu Qiao</a> (SIAT)
</div>
</div>
</div>


<div style="clear:both;">
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<hr>
<div id="clustrmaps-widget"></div><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=1Moc&d=eOIH5SiFBexbBlRqzMtY91KtangZBT_tySQbrlrZEqE"></script>

</body>
</html>
